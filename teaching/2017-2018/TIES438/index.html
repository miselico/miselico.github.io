
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>TIES438- Big Data Engineering - Michael Cochez</title>
  <meta name="author" content="">

  
  <meta name="description" content=" ">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://localhost:4000/teaching/2017-2018/TIES438/">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="" rel="alternate" title="Michael Cochez" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="./javascripts/libs/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-54043599-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body   class="no-sidebar"  >
  <header role="banner"><hgroup>
  <h1><a href="/">Michael Cochez</a></h1>
  
    <h2>Assistant Professor at Vrije Universiteit Amsterdam</h2>
  
</hgroup>

</header>
  <nav role="navigation"><!--
<ul class="subscription" data-subscription="rss">
  <li><a href="" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
-->
  
<ul class="main-navigation">
  <li><a href="/">Home</a></li>
  <li><a href="/research/">Research</a></li>
  <li><a href="/teaching/">Teaching</a></li>  
  <li><a href="/software/">Software</a></li>
  <li><a href="/contact/">Contact</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div>
<article role="article">
  
  <header>
    <h1 class="entry-title">TIES438- Big Data Engineering</h1>
    
  </header>
  
  <p><strong>December 2017 II</strong></p>

<h2 id="lectures">Lectures</h2>

<p>(each lecture is two blocks of two lecturing hours, i.e., each lecture is 4 contact hours)</p>

<ul>
  <li>Lecture 1 : <a href="lectures/introduction/">Introduction lecture</a> A simple example of an <a href="lectures/introduction/Hamming.java">approximate algorithm for Hamming distance</a>
    <ul>
      <li>Topics: Distance metrics, min-hashing, random hyperplane hashing, combing rows and bands</li>
      <li>Sources:
        <ul>
          <li>Mining massive datasets (mmds, see literature below), chapter 1, chapter 3</li>
          <li>https://people.csail.mit.edu/indyk/p117-andoni.pdf (except for Euclidean distance)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Lecture 2:
    <ul>
      <li>Topics: Locality-sensitive hashing, LSH forest, hashing overview, birthday paradox, preprocessing text for distance computation</li>
      <li>Sources:
        <ul>
          <li>Mining massive datasets (mmds, see literature below), chapter 1, chapter 3</li>
          <li>https://people.csail.mit.edu/indyk/p117-andoni.pdf (except for Euclidean distance)</li>
          <li>The information on hash functions comes from various sources, the most important information is in the sources above.  A hash function recommended to use for general (non-secure) hashing is murmur3F (see <a href="https://github.com/aappleby/smhasher/blob/master/src/MurmurHash3.cpp">source</a> ), which is also available in, for example, the Guava library for Java.</li>
        </ul>
      </li>
      <li>Topics: Data streams, stream sampling (fixed size, fraction), windows (sliding, tumbling), bloom filters</li>
      <li>Sources:
        <ul>
          <li>mmds chapter 4</li>
          <li>The <a href="https://en.wikipedia.org/wiki/Bloom_filter">wikipedia article</a> on bloom filters is well written</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Lecture 3:
    <ul>
      <li>Topics: Dacaying windows, Counting distinct values: sampling, K-min values, Flajolet Martin 1983, LogLog, HyperLogLog</li>
      <li>Sources:
        <ul>
          <li>mmds chapter 4</li>
          <li>https://research.neustar.biz/2012/07/09/sketch-of-the-day-k-minimum-values/</li>
          <li>https://research.neustar.biz/2012/10/25/sketch-of-the-day-hyperloglog-cornerstone-of-a-big-data-infrastructure/</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Lecture 4:
    <ul>
      <li>Topics: Curse of dimensionality, clustering, K-means, hierarchical clustering, density based clustering (DBSCAN), CURE, Twister Tries</li>
      <li>Sources:
        <ul>
          <li>mmds chapter 7</li>
          <li>http://mathworld.wolfram.com/HypercubeLinePicking.html</li>
          <li>The basic principles are well explained in the wikipedia article on DBSCAN</li>
          <li>http://users.jyu.fi/~miselico/papers/twister_tries.pdf</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Lecture 5:
    <ul>
      <li>Big data ar rest: storage and processing</li>
      <li><a href="lectures/data_at_rest/lecture_adapted_storage.pptx">Storage slides</a></li>
      <li><a href="lectures/data_at_rest/lecture_adapted_processing.pptx">Processing slides</a></li>
      <li>Also useful in this context are the sections 2.1, 2.2, and 2.4 of the mmds book.</li>
      <li>More details on the foundations of Spark can be found fromt the <a href="https://www.usenix.org/node/162809">usenix paper</a>.</li>
    </ul>
  </li>
  <li>Lecture 6:
    <ul>
      <li>Topics: graphs, PageRank, Personalized PageRank (PPR), Bookmark Coloring Algorithm (BCA), Graph Embeddings (TransE, global embeddings)</li>
      <li>Sources:
        <ul>
          <li>graphs, PageRank : mmds chapter 5 (section 5.1) An explanation similar to the one in the lecture can be found in the online course.</li>
          <li>PPR is called topic-sensitive pagerank in mmds section 5.3 (not covered 5.3.4)</li>
          <li>BCA (this paper also has an explanation of PPR) : Berkhin, Pavel. “Bookmark-coloring algorithm for personalized pagerank computing.” Internet Mathematics 3.1 (2006): 41-62.</li>
          <li>TransE: we only covered the basic idea behind translational embeddings and the optimization function. Paper : Bordes, Antoine, et al. “Translating embeddings for modeling multi-relational data.” Advances in neural information processing systems. 2013.</li>
          <li>Global embeddings: We only covered the overall idea from the <a href="https://docs.google.com/presentation/d/e/2PACX-1vSjFiMG9XuzDpGXoU-yu7YTJrpQ-P2-TFVgvSehsgHz-u0xLG5_VL51Qmm9lbc0HbG30Yeh9CHW7hNj/pub?start=false&amp;loop=false">slides</a>.
  Paper with all details: Cochez, M., Ristoski, P., Ponzetto, S. P., &amp; Paulheim, H. (2017). Global RDF Vector Space Embeddings. In C. d’Amato, et. al (Eds.), ISWC 2017 - The Semantic Web : 16th International Semantic Web Conference, Proceedings, Part I (pp. 190-207). Lecture Notes in Computer Science, 10587. Cham: Springer.[<a href="https://jyx.jyu.fi/dspace/handle/123456789/56299">link</a>]</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="exercises-there-will-be-3-exercises-altogether">Exercises (there will be 3 exercises altogether.)</h2>
<p>(See also course completion modes below)</p>
<ul>
  <li>
    <p>Exercise 1 : Locality-sensitive hashing for nearest neighbor finding - classifiers. <a href="exercises/exercise1/">Finding Similar Items - Fighting Spam</a> [Due 15 January]</p>
  </li>
  <li>
    <p>Exercise 2: Estimating the number of distinct items in a stream. <a href="exercises/exercise2/">Approximating Stream Cardinalities</a> [Due 15 January]</p>
  </li>
  <li>
    <p>Exercise 3: <a href="exercises/exercise3/">Analyzing Graphs using Spark</a> [Due 15 January]</p>
  </li>
</ul>

<h2 id="individual-task">Individual task</h2>
<ul>
  <li>The individual task is the summarization of a research paper related to the Big Data Engineering field. <a href="exercises/articleSummary/">Instructions</a> [Due 15 January]</li>
</ul>

<h2 id="contents">Contents:</h2>

<p>During the course multiple facets related to the Big Data phenomenon will be studied. First, students will get introduced to large data sets and streaming data. Then, example storage solutions and processing algorithms will be studied. Finally, we will look into hardware considerations and apply the theory on real world datasets related to news, wikipedia, brain analysis, biology, chemistry, etc.</p>

<p>Students who wish to work on a problem specific to their own research should discuss this with the teacher at the beginning of the course.</p>

<h2 id="learning-outcomes">Learning outcomes:</h2>

<p>After completion of this course the students will understand the concepts related to, and the intrinsic characteristics of big amounts of data. 
The student will then be able to evaluate algorithms and technology to deal with problems in which big amounts of data are involved.</p>

<h2 id="prerequisites">Prerequisites:</h2>

<p>The student should know how to program (at least programming 2) and be familiar with  algorithms, data structures and computational complexity. Further, the student should have notion of sets, probability theory, linear algebra, and statistics.</p>

<h2 id="modes-of-study">Modes of study:</h2>

<p>Students should attend the lectures and read the assigned materials. Further, the implementation of algorithms is intended to assist the students in their understanding of the course content.</p>

<h2 id="completion-mode">Completion mode:</h2>

<p>The course is completed by either:</p>

<ul>
  <li>implementing the assigned tasks (individually or as a pair) and individually write a summary of one research paper. <strong>OR</strong></li>
  <li>passing the exam at the end of the intensive course or at a later point in the next semester.</li>
</ul>

<h2 id="literature">Literature:</h2>
<ul>
  <li>Mining massive data sets - Anand Rajaraman, Jure Leskovec, Jeffrey D. Ullman free download from <a href="http://www.mmds.org/">http://www.mmds.org/</a></li>
  <li>The online course on the same book (see <a href="https://www.mmds.org">https://www.mmds.org</a> ) overlaps partially with the course.</li>
  <li>Optional: Motwani, and Raghavan. Randomized Algorithms. Cambridge, UK: Cambridge University Press, 1995. ISBN: 0521474655. (available in JYU trough EBSCOhost https://jyu.finna.fi/Record/jykdok.1485577 )</li>
  <li>Optional: Data Clustering: Algorithms and Applications - Charu C. Aggarwal, Chandan K. Reddy 2013 by Chapman and Hall/CRC ISBN 9781466558212</li>
</ul>

<h2 id="master-thesis-topics">Master thesis topics</h2>

<p>See <a href="../supervision/">the master thesis supervision</a> page</p>

<h1 id="links">Links</h1>
<ul>
  <li>Course information in Korppi <a href="https://korppi.jyu.fi/kotka/course/student/generalCourseInfo.jsp?course=198879">TIES438</a></li>
</ul>


  
    <footer>
      
      
        <div class="sharing">
  
  
  
</div>

      
    </footer>
  
</article>

</div>


    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2020 - Michael Cochez -
  <span class="credit">Powered by Jekyll. Derived from Octopress.</span>
</p>

</footer>
  











</body>
</html>
